{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiYUF679ewIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Importa/Instala y Obertura\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vFJLnmalUIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (Nada) Importa y Puente\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from transformers import *\n",
        "\n",
        "# Estructura en tuplas para los modelos, con su tokenizer y sus preentrenados \n",
        "#          Model          | Tokenizer          | Pretrained weights shortcut\n",
        "#(BartModel, BartTokenizer, 'bart-large-mnli') Tupla Bart\n",
        "#(TFFlaubertWithLMHeadModel, FlaubertTokenizer, 'flaubert-base-cased') Tupla Flaubert TensorFlow\n",
        "#(TransfoXLLMHeadModel,  TransfoXLTokenizer,  'transfo-xl-wt103') Tupla TXL\n",
        "#(FlaubertWithLMHeadModel, FlaubertTokenizer, 'flaubert-base-cased') Tupla Flaubert\n",
        "#(GPT2DoubleHeadsModel, GPT2Tokenizer , 'gpt2-medium') Tupla GTP2\n",
        "\n",
        "# Carga el preentrenado con el modelo/tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-large', pad_token_id=tokenizer.eos_token_id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XLyqk5Eitw1V",
        "colab": {}
      },
      "source": [
        "\n",
        "# Codifica textos para cada estructura, almacena en un arreglo y Coro\n",
        "# (esto se debe mejorar para que sea recursivo, eso lo hare(mos)... eventualmente)\n",
        "#self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_specific_kwargs\n",
        "#tf.random.set_seed(3)\n",
        "\n",
        "INPUT = \"he realizes it's late and goes to school\"\n",
        "input_ids = tokenizer.encode(INPUT, add_space_before_punct_symbol=True, return_tensors='pt')\n",
        "beam_outputs=model.generate(input_ids,\n",
        "                            max_length=len(INPUT),\n",
        "                            top_k=20,\n",
        "                            top_p=0.98,\n",
        "                            num_beams=50,\n",
        "                            num_return_sequences=5,\n",
        "                            no_repeat_ngram_size=2,\n",
        "                            repetition_penalty=4,\n",
        "                            do_sample=False\n",
        "                            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYgjWbdvv1ew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imprime y Fade Out\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "  print(\"Modelo:\",i)\n",
        "  print(tokenizer.decode(beam_output, skip_special_tokens=True))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}