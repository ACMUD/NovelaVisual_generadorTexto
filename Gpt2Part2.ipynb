{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiYUF679ewIy",
        "colab_type": "code",
        "outputId": "4acc0202-d81e-4d47-c398-b49afa074873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "\n",
        "# Importa/Instala y Obertura\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-moozxp4i\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-moozxp4i\n",
            "Requirement already satisfied (use --upgrade to upgrade): transformers==2.10.0 from git+https://github.com/huggingface/transformers in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.10.0) (1.18.4)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers==2.10.0) (0.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==2.10.0) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.10.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.10.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.10.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.10.0) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.10.0) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.10.0) (0.0.43)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==2.10.0) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==2.10.0) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==2.10.0) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.10.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.10.0) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.10.0) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.10.0) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.10.0) (0.15.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.10.0) (7.1.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.10.0-cp36-none-any.whl size=667026 sha256=4fe2a774b3048a6f75f70acaec9bae1747d49c1ac88528128e1c2ee87df2ee91\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dsjqyh5f/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "Successfully built transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vFJLnmalUIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (Nada) Importa y Puente\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from transformers import *\n",
        "\n",
        "# Estructura en tuplas para los modelos, con su tokenizer y sus preentrenados \n",
        "#          Model          | Tokenizer          | Pretrained weights shortcut\n",
        "#(BartModel, BartTokenizer, 'bart-large-mnli') Tupla Bart\n",
        "#(TFFlaubertWithLMHeadModel, FlaubertTokenizer, 'flaubert-base-cased') Tupla Flaubert TensorFlow\n",
        "#(TransfoXLLMHeadModel,  TransfoXLTokenizer,  'transfo-xl-wt103') Tupla TXL\n",
        "#(FlaubertWithLMHeadModel, FlaubertTokenizer, 'flaubert-base-cased') Tupla Flaubert\n",
        "#(GPT2DoubleHeadsModel, GPT2Tokenizer , 'gpt2-medium') Tupla GTP2\n",
        "\n",
        "# Carga el preentrenado con el modelo/tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-large', pad_token_id=tokenizer.eos_token_id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XLyqk5Eitw1V",
        "colab": {}
      },
      "source": [
        "\n",
        "# Codifica textos para cada estructura, almacena en un arreglo y Coro\n",
        "# (esto se debe mejorar para que sea recursivo, eso lo hare(mos)... eventualmente)\n",
        "#self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_specific_kwargs\n",
        "#tf.random.set_seed(3)\n",
        "\n",
        "INPUT = \"he realizes it's late and goes to school\"\n",
        "input_ids = tokenizer.encode(INPUT, add_space_before_punct_symbol=True, return_tensors='pt')\n",
        "beam_outputs=model.generate(input_ids,\n",
        "                            max_length=len(INPUT),\n",
        "                            top_k=20,\n",
        "                            top_p=0.98,\n",
        "                            num_beams=50,\n",
        "                            num_return_sequences=5,\n",
        "                            no_repeat_ngram_size=2,\n",
        "                            repetition_penalty=4,\n",
        "                            do_sample=False\n",
        "                            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYgjWbdvv1ew",
        "colab_type": "code",
        "outputId": "719efb93-7bb0-4b81-be17-e44face7945a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# Imprime y Fade Out\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "  print(\"Modelo:\",i)\n",
        "  print(tokenizer.decode(beam_output, skip_special_tokens=True))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Modelo: 0\n",
            "he realizes it's late and goes to school for the rest of his life.\n",
            "Modelo: 1\n",
            "he realizes it's late and goes to school the next day. When he gets home, his mother tells him that she is going out of town for a few days so they can spend more time together\n",
            "Modelo: 2\n",
            "he realizes it's late and goes to school the next day. When he gets home, his mother tells him that she is going out of town for a few days so they can spend some time together\n",
            "Modelo: 3\n",
            "he realizes it's late and goes to school the next day. When he gets home, his mother tells him that she is going out of town for a few days so they can go on vacation together\n",
            "Modelo: 4\n",
            "he realizes it's late and goes to school. When he gets home, his mother tells him that she is going out of town for a few days so they can have some time alone together before her\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}